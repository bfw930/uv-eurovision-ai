{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' imports '''\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import (\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    GPT2Config,\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LineByLineTextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, file_path: str, block_size = 512):\n",
    "\n",
    "        # no feature cache\n",
    "        with open(file_path, encoding = 'utf-8') as f:\n",
    "\n",
    "            lines = [line for line in f.read().splitlines()\n",
    "                     if (len(line) > 0 and not line.isspace())]\n",
    "\n",
    "        self.examples = tokenizer.batch_encode_plus(\n",
    "            lines, add_special_tokens = True, max_length = block_size)['input_ids']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return torch.tensor(self.examples[i], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' config parameters '''\n",
    "\n",
    "# Optional input sequence length after tokenization.\n",
    "# The training dataset will be truncated in block of this size for training\n",
    "# Default to the model max input length for single sentence inputs (take into account special tokens)\n",
    "block_size: int = -1\n",
    "\n",
    "# Number of updates steps to accumulate before performing a backward/update pass\n",
    "gradient_accumulation_steps: int = 1\n",
    "\n",
    "# Max gradient norm\n",
    "max_grad_norm: float = 1.\n",
    "\n",
    "# If > 0: set total number of training steps to perform. Override num_train_epochs\n",
    "max_steps: int = -1\n",
    "\n",
    "# Linear warmup over warmup_steps\n",
    "warmup_steps: int = 0\n",
    "\n",
    "\n",
    "''' init train env '''\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = 1\n",
    "\n",
    "\n",
    "# Set seed\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Load pretrained model and tokenizer '''\n",
    "\n",
    "# load pretrained model and tokenizer\n",
    "config_class, model_class, tokenizer_class = GPT2Config, GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# init pretrained tokeniser\n",
    "tokenizer = tokenizer_class.from_pretrained('gpt2', cache_dir = None)\n",
    "\n",
    "\n",
    "# Our input block size will be the max possible for the model\n",
    "if block_size <= 0:\n",
    "    block_size = tokenizer.max_len\n",
    "else:\n",
    "    block_size = min(block_size, tokenizer.max_len)\n",
    "\n",
    "\n",
    "# init config\n",
    "config = config_class()\n",
    "\n",
    "# Training new model from scratch\n",
    "#model = model_class(config = config)\n",
    "\n",
    "model = model_class.from_pretrained('gpt2', config = config)\n",
    "\n",
    "# push model to device\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' init dataset '''\n",
    "\n",
    "# get training dataset\n",
    "#file_path = '../data/lyrics/eurovision-lyrics-lines.txt'\n",
    "#file_path = '../data/lyrics/wikitext-2/wiki.train.tokens'\n",
    "#file_path = '../data/lyrics/eurovision-lyrics-en-lines'\n",
    "#file_path = '../data/lyrics/eurovision-lyrics-lines-full'\n",
    "file_path = '../data/lyrics/eurovision-lyrics-en-lines-lrg'\n",
    "\n",
    "train_dataset = LineByLineTextDataset(\n",
    "    tokenizer, file_path = file_path, block_size = block_size)\n",
    "#train_dataset = TextDataset(tokenizer, args, file_path=file_path, block_size = block_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" init dataloader, optimiser \"\"\"\n",
    "\n",
    "train_batch_size = 1\n",
    "\n",
    "def collate(examples: List[torch.Tensor]):\n",
    "\n",
    "    if tokenizer._pad_token is None:\n",
    "\n",
    "        return pad_sequence(examples, batch_first = True)\n",
    "\n",
    "    return pad_sequence(examples, batch_first = True, padding_value = tokenizer.pad_token_id)\n",
    "\n",
    "# init random sampler on dataset\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "\n",
    "# init dataloader\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, sampler = train_sampler, batch_size = train_batch_size, collate_fn = collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' init optimiser, learning rate scheduler '''\n",
    "\n",
    "# The initial learning rate for Adam\n",
    "learning_rate: float = 5e-5\n",
    "\n",
    "# Weight decay if we apply some\n",
    "weight_decay: float = 0.\n",
    "\n",
    "# Epsilon for Adam optimizer\n",
    "adam_epsilon: float = 1e-8\n",
    "\n",
    "\n",
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": weight_decay,\n",
    "    },\n",
    "    {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "]\n",
    "\n",
    "# init optimiser on model parameters\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr = learning_rate, eps = adam_epsilon)\n",
    "\n",
    "\n",
    "\n",
    "# set training epochs\n",
    "num_train_epochs = 3\n",
    "\n",
    "# get total steps\n",
    "if max_steps > 0:\n",
    "    t_total = max_steps\n",
    "    num_train_epochs = max_steps // (len(train_dataloader) // gradient_accumulation_steps) + 1\n",
    "else:\n",
    "    t_total = len(train_dataloader) // gradient_accumulation_steps * num_train_epochs\n",
    "    \n",
    "\n",
    "# init learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps = warmup_steps, num_training_steps = t_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 4.8972459363937375\n",
      "100 4.8972459363937375\n",
      "200 4.803398907184601\n",
      "300 4.768976623217265\n",
      "400 4.656124718338251\n",
      "500 4.624998973727227\n",
      "600 4.584139998654525\n",
      "700 4.567651105182511\n",
      "800 4.5199873229116205\n",
      "900 4.469655289848646\n",
      "1000 4.4295827458500865\n",
      "1100 4.395757151625373\n",
      "1200 4.381694662968318\n",
      "1300 4.352846398170178\n",
      "1400 4.322022956780025\n",
      "1500 4.290419712305069\n",
      "1600 4.260088528208435\n",
      "1700 4.252467391455875\n",
      "1800 4.238117224607203\n",
      "1900 4.2141116218504155\n",
      "2000 4.190335895985365\n",
      "2100 4.17041817341532\n",
      "2200 4.148430817113681\n",
      "2300 4.1250156825132995\n",
      "2400 4.115036492608487\n",
      "2500 4.094824813306332\n",
      "2600 4.094347468282168\n",
      "2700 4.082521997149344\n",
      "2800 4.078683818908674\n",
      "2900 4.060866205312054\n",
      "3000 4.048018341590961\n",
      "3100 4.0334402210770115\n",
      "3200 4.019374075257219\n",
      "3300 4.0105228100626755\n",
      "3400 3.997905661897624\n",
      "3500 3.987107959708997\n",
      "3600 3.9669236762118008\n",
      "3700 3.9563943721474826\n",
      "3800 3.9418052757256907\n",
      "3900 3.9322917248958196\n",
      "4000 3.923089219920337\n",
      "4100 3.9204962643958265\n",
      "4200 3.9098714473608527\n",
      "4300 3.900450464387879\n",
      "4400 3.889471711067229\n",
      "4500 3.8811698776793975\n",
      "4600 3.872313364301606\n",
      "4700 3.8598136285815308\n",
      "4800 3.8532459043257403\n",
      "4900 3.841985547175745\n",
      "5000 3.8355828560929743\n",
      "5100 3.8296987095164754\n",
      "5200 3.821077714918826\n",
      "5300 3.810637923991835\n",
      "5400 3.8028001830548575\n",
      "5500 3.790633041782495\n",
      "5600 3.783070874400125\n",
      "5700 3.771841016850667\n",
      "5800 3.7623305505202835\n",
      "5900 3.7550282132151236\n",
      "6000 3.7527176001714833\n",
      "6100 3.749997255733479\n",
      "6200 3.7405043838675147\n",
      "6300 3.733716718733989\n",
      "6400 3.7280361746528343\n",
      "6500 3.7240690278454163\n",
      "6600 3.7155392914917584\n",
      "6700 3.6993765745331695\n",
      "6800 3.691531130049458\n",
      "6900 3.685956496771558\n",
      "7000 3.6770358201422577\n",
      "7100 3.6731821505775475\n",
      "7200 3.66734711362129\n",
      "7300 3.6627713197620495\n",
      "7400 3.6512662414469164\n",
      "7500 3.64688150158204\n",
      "7600 3.6394744083158885\n",
      "7700 3.6328583328949375\n",
      "7800 3.626256072838434\n",
      "7900 3.61479413687155\n",
      "8000 3.606551754866821\n",
      "8100 3.59891549514225\n",
      "8200 3.58815732389834\n",
      "8300 3.581807119765197\n",
      "8400 3.574434701180885\n",
      "8400 3.574434701180885\n",
      "8500 3.5663531293067874\n",
      "8600 3.5595216107241563\n",
      "8700 3.5545210071005284\n",
      "8800 3.549147148404875\n",
      "8900 3.5372162189409253\n",
      "9000 3.5358540990385032\n",
      "9100 3.5326951761989265\n",
      "9200 3.5288700150727084\n",
      "9300 3.522745287627494\n",
      "9400 3.515280412736162\n",
      "9500 3.5114751241405457\n",
      "9600 3.5072037021693845\n",
      "9700 3.500987368184796\n",
      "9800 3.494845547623088\n",
      "9900 3.4892663143652625\n",
      "10000 3.4825765186541537\n",
      "10100 3.477204809777587\n",
      "10200 3.4715504436341726\n",
      "10300 3.4617871405837835\n",
      "10400 3.4557188101101035\n",
      "10500 3.4497144150594528\n",
      "10600 3.443950325425188\n",
      "10700 3.4391096566814907\n",
      "10800 3.4370069787066044\n",
      "10900 3.428041243961347\n",
      "11000 3.418922367716785\n",
      "11100 3.412931621477143\n",
      "11200 3.4088879384869455\n",
      "11300 3.401754416246199\n",
      "11400 3.3967459501186235\n",
      "11500 3.389948531613306\n",
      "11600 3.3840063505793836\n",
      "11700 3.3798865713504527\n",
      "11800 3.375609651631661\n",
      "11900 3.3690075372516994\n",
      "12000 3.3630845687783486\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c957d30ff966>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# push data to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "''' perform training '''\n",
    "\n",
    "global_step = 0\n",
    "epochs_trained = 0\n",
    "steps_trained_in_current_epoch = 0\n",
    "\n",
    "tr_loss, logging_loss = 0.0, 0.0\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# zero gradients\n",
    "model.zero_grad()\n",
    "\n",
    "# iterate epochs\n",
    "for epoch in range(num_train_epochs):\n",
    "\n",
    "    # get data batch from dataloader\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Skip past any already trained steps if resuming training\n",
    "        if steps_trained_in_current_epoch > 0:\n",
    "            steps_trained_in_current_epoch -= 1\n",
    "            continue\n",
    "\n",
    "\n",
    "        # unpack batch data\n",
    "        inputs, labels = (batch, batch)\n",
    "\n",
    "        # push data to device\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "\n",
    "        # set model to train\n",
    "        model.train()\n",
    "\n",
    "        try:\n",
    "        \n",
    "            # perform forward pass through model\n",
    "            outputs = model(inputs, labels=labels)\n",
    "\n",
    "            # obtain loss; model outputs are always tuple in transformers\n",
    "            loss = outputs[0]\n",
    "\n",
    "            # gradient accumulation\n",
    "            if gradient_accumulation_steps > 1:\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "\n",
    "            # backprop loss\n",
    "            loss.backward()\n",
    "\n",
    "            # store loss\n",
    "            tr_loss += loss.item()\n",
    "\n",
    "\n",
    "            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "\n",
    "                # perform gradient normalisation\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "                # step optimiser\n",
    "                optimizer.step()\n",
    "\n",
    "                # Update learning rate schedule\n",
    "                scheduler.step()\n",
    "\n",
    "\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        if global_step % 100 == 0:\n",
    "            print(global_step, tr_loss / global_step)\n",
    "\n",
    "        if max_steps > 0 and global_step > max_steps:\n",
    "            break\n",
    "    if max_steps > 0 and global_step > max_steps:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' saving '''\n",
    "\n",
    "model.save_pretrained('../data/lyrics/model-lyrics-best')\n",
    "#tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a trained model and vocabulary that you have fine-tuned\n",
    "model = model_class.from_pretrained('../data/lyrics/model-en-lyrics-03/')\n",
    "#tokenizer = tokenizer_class.from_pretrained(args.output_dir)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Thank you\n",
      " We're the only ones left to rock and roll forever more. Oh yea\n",
      " I’ll fly over it once again\n",
      " Love your love\n",
      " Love! (Love!) Love! Love! Love! Love! Love\n",
      " (You're the one I love) Oh\n",
      " Rock! Rock! Rock! Rock! Rock! Rock! Rock! Roc\n",
      " What are you doing? We’re going up! Why do w\n",
      " I'm gonna shine a light in every corner of my dreams\n",
      " Beautiful\n",
      " What a wonderful world we've got to have a love about? – W\n",
      " – Hallelujah for everybody! (Chorus 2000) (Fl\n",
      " Love! Love! Love! (Love!) - Aphrodite Lis\n",
      " love! love! love! love! love! love! love! lov\n",
      " Love! Love! Love! Love! Love! Love! (Let\n",
      " Thank you\n",
      " I love you! I love you! I love you! You love m\n",
      " Love! Love! Love! Love! Love! Love! Love! Lov\n",
      " I need you\n",
      " Love! Love! Love! Love! Love! Love! Love! Lov\n",
      " – rainbow (Ukraine 1984) – Kingdom of Love (Sloveni\n",
      " We've never seen them separation! (And it has never been…) Di\n",
      " I'm gonna go 'cause I just wanna sing you… To lov\n",
      " I’m a puppet on a string\n",
      " Alone\n",
      " Love will survive; love will survive;...we’ll survive\n",
      " Love! Love! Love! Love! Love! Love! Love! Lov\n",
      " Love\n",
      " (Bring me back to your feet) Ooh… love\n",
      " Thank you\n",
      " Cause you keep me hanging on my very own rhythm. Only yo\n",
      " That kiss tonight at night will wake me up\n",
      " (Are you ready to take my hand?) 'Cause you're the onl\n",
      " We hardly seem to rock ‘n’ roll anymore\n",
      " I hope your love can do all these things that you never change.\n",
      " Love! Love! Love! Love! Love! Love! Love! \n",
      " Aphrodite – You Might Be the Love Devil (United Kingdom 2008\n",
      " (Congratulations for the challenge) Thank you\n",
      " You’ll be sorry to yourself\n",
      " Love! Love! Love! Love! Love! Love! Love! Lov\n",
      " We have never stopped loving! We have never stopped loving! That is th\n",
      " (Congratulations!) oh love (Congratulations for the challenge) rainbow love! darlin\n",
      " Love! Love! Love! Love! Love! (Love!) (Lov\n",
      " – Amina (Slovenia 2008) (translation) \n",
      " Oh\n",
      " What’s gone between us is now clear and true? (Wh\n",
      " I just want you to tell me what's happening? (You're doin\n",
      "Love keeps us alive and we keep us alive forever more\n",
      " (Singing out for you) One kiss for me forever more… \n",
      " Love! Love! Love! Love! Love! Love! Love! Lov\n",
      " Love! Love! Love! Love! (Why are we together? Lov\n",
      " Love! Love! Love! (Love!) Love! Love! Love\n",
      " Love forever and everyone will find me again in love forevermore! More an\n",
      " Don't say you love me\n",
      " Oh… rock hallelujah!… (Congratulations for the challenge\n",
      " Rock! Rock\n",
      " Oh\n",
      " (Congratulations for the challenge!) Congratulations for the challenge! Congratulations fo\n",
      " We have to laugh\n",
      " Yeah\n",
      " Love! Love! Love! Love! Love! Love! Love! Lov\n",
      " What the heck is happening between us? What we're doing between us\n",
      " Love! Love! Love! Love! Love! Love! Love! Lov\n",
      " she put me on fire! hallelujah! darling\n",
      " I think it’s over! Just let it persist and persis\n",
      " We got love (Love!) – Good Bye Bye (Went Your Wa\n",
      " Come come and take me now\n",
      " Diva is proudly engaged to the bone! Diva's walls are i\n",
      " peace! the music of the earth has arrived\n",
      " With my people they were better than us! We've gotta speed it u\n",
      " break your heart! their flames are vivid candles of hope\n",
      " A fight over it all is over! Has come my way now! I\n",
      " - The Capricious Disrupade (Estonia 2007) \n",
      " Euphrates! Euphrates\n",
      " Love won’t break us down\n",
      " We're dancing in the middle of the night (Stupid’\n",
      " game\n",
      " (Congratulations for the challenge)Congratulations for the challenge\n",
      " spoke! vim! sativa! turbana! turbana\n",
      " We found it because we were as we are! We discovered it when w\n",
      " Ready to rock… everyone is gonna rock 'n' roll for you\n",
      " - Verona (Dai na\n",
      " Lights up! Whoa… and… all that is yet to come\n",
      " northern lights\n",
      " Just let is persist! It’s bigger than everything we se\n",
      " love\n",
      " We'll be together again\n",
      " Euphoria! Amsterdam! Amsterdam! We played Eurovision in the arms o\n",
      " Quiet! A little peace\n",
      " Vote for peace! Vote for peace!Vote for a peace! Vote fo\n",
      " It's raining a heavy rain like heavy raindrops on me like grey cloud\n",
      " Revives! Revives! Revives! Revives! Revives\n",
      " Aphrodite\n",
      " My rainbow\n",
      " united\n",
      " - Everyday (Heaven knows) (Congratulations for today) (Congratulations fo\n",
      " – I’m trembling in my arms (Bravo 2013\n",
      " (Without feeling) all kinds of everything remind me of you. – Yo\n",
      " Revives! (Love! - crippled) Revives our dream! \n",
      " be aware of yourself\n",
      " war! war! war! war! war! war! war! wa\n",
      " We're together now\n",
      " Two of us\n",
      " Oh\n",
      " peace! bring me to an end! break the rhythm and pace this rhyth\n",
      " Ver! (Congratulations!) – Years After He Quit (United Kingdom 1996\n",
      "peace peace! peace! peace! peace! peace! peace! peace\n",
      " proudly\n",
      " Happy ending\n",
      " Three words at a time\n",
      " It seems like a little song with me only a little idea\n",
      " We're running\n",
      " moving forever over you… gone over in the mind… or you forgot \n",
      " It's gonna be 'til we die we die'\n",
      " We got your love\n",
      " Rain! Don't let the rain fall… Hallelujah! \n",
      " Euphoria! Euphoria! Euphoria! Euphoria! Euphoria\n",
      " It won’t be anymore hard\n",
      " peace! Euphrates rises\n",
      " Whoa... nevermind your pleas\n",
      " We gotta speed it up\n",
      " That means we ain’t brokein’ down we angel'\n",
      " Euphrates for all those who have so much to say? Euphrates fo\n",
      "Let it swing\n",
      " In my arms\n",
      " Soul and spirit are here\n",
      " We won’t let you go against us\n",
      " We're gonna rap-op-bop\n",
      " Bunny\n",
      " Turn the sound of silence into trembling music! Hallelujah! \n",
      " Love can be broken just one more thing\n",
      " We will be the heroes of our time\n",
      " Cause it’s only a dream and illusion\n",
      " That is the power of a song that is heard by a great music compose\n",
      " Two worlds we were born in? Were we the same? Tel\n",
      " Ooh-oh-pop\n",
      " A troubadour and quavers\n",
      " It beats to the sound of silence in my sleep tonight\n",
      " Squadronrons of stars\n",
      " (Ooh… rock\n",
      " Don't give up! – Rock is our only option\n",
      " Spring! Let them break the silence between you and me forever more. \n",
      " Oh\n",
      " Has it been enough? Was it too soon? Was it too late fo\n",
      " If you fly it\n",
      " Why is it burning? Cause I can’t? Caus\n",
      " Fire! Fire! God! Fire of love! It’s bigge\n",
      " (Congratulations for the challenge!) That was once upon a time! – \n",
      "... Has no fear\n",
      " Fire of tomorrow! Fire of tomorrow! Fire of tomorrow! Fire of tomorro\n",
      " It's bigger than everything we see on TV\n",
      " We're dancing…now! (Stunning) Oh…now! W\n",
      " Oh... roll... roll till you lose all control? Roll till you los\n",
      " - Trigger (Another arah) (Israel 2007) (Israel 2008\n",
      " We are born to love you once again\n",
      " Rock and roll\n",
      " Are you ready? – Oh\n",
      " United Kingdom – Ferb – The Bottomless Heroine (Ukraine 2008\n",
      " Chorus movement\n",
      " It seems like there’s one thing I must say before I g\n",
      " Come on\n",
      " Rollin' round and round and round and round and round and round an\n",
      " We never seem to rock\n",
      " Fire of love! Fire of love! Fire of love! Fire of lov\n",
      " 'Cause we're together! 'Cause we're everywhere at once! \n",
      " Scary plans let us pray by yourself sleeping inside a dream\n",
      " Premiership – Celtics Are Young (Chrysaubia 2006) When yo\n",
      " Rock! Rock! Rock! Rock! Rock! Rock! Rock! Roc\n",
      " May dawn come true for you inside this woman of rising for silver from th\n",
      " My love! Has no limit to what can I can dream of you\n",
      " We will dance and laugh until our souls rest. 'til our hearts chang\n",
      " (Let’s go!)\n",
      "Oh… gooh… (O\n",
      " Roll! 'n' roll 'n' roll 'n' roll\n",
      " Roll now! Roll now! Roll now! Roll now! Roll now\n",
      " – We've never stopped believing in our dreams. 'cause we grew u\n",
      " It seems that one knows how to laugh! What about music? Rock \n",
      " Fire of love! Fire of love! Fire of love! Fire of lov\n",
      " Rock\n",
      " I will soon be back for more\n",
      " Rescue us strong! Rock ‘n’ roll with the lanc\n",
      " Fire of love! Tell me now! Cause I’\n",
      " Fire! Fire of love! Fire of love! Fire of love! Fir\n",
      " Oh\n",
      " Infantry! Rock 'n' roll! Hallelujah for \n",
      " We will get it right back again for good? Yeah\n",
      " Rock ‘n’ roll… rock ‘n’ rol\n",
      " Build a tower\n",
      " That waits till we die and then we die! I wonder why? \n",
      " Fire! Fire! Heaven's leading me in a thousand pieces of voice\n",
      " Rock\n",
      " If you’re going up\n",
      " Fire! Fire! Fire of love! Fire of love! Fire of lov\n",
      " Save us! Save us! Fly! Fly! Fly\n",
      "Keep it moving slowly\n",
      " We'll sing together! – We'll sing together! (Fly Together!\n",
      " Fly! (Fly!) Fly! (Fly!)Fly! Baby\n",
      " A little closer\n",
      " Where are you at? (Where are you?) – Rock 'n\n",
      " Come and give me a kiss from the sky like you are made of star\n",
      " - Malta 2009\n",
      "…It was impossible to believe that there's something between us anymore…Wh\n",
      "’s flying over the horizon; couldn’t escape if \n",
      " It’s bigger than everything we see or hear or feel in ou\n",
      " I don’t care about your bones or your feet\n",
      " flying over the city\n",
      "... sea breezes\n",
      " Come and enjoy! Fly away\n",
      " Love's staying strong in my heart! - Love’ful but the\n",
      " Look\n",
      " (Fly\n",
      " –Jumpin' up! –Let’s go… (Fl\n",
      " We're dancing to a song never left behind us! (Congratulations!) W\n",
      " We love life together! -Why The Rain? - Rock bottom (Fl\n",
      " You’re my sacred passion and I have no other option othe\n",
      " My heartache and my pain go up! Three more nights! We'l\n",
      " Flag! Come and kiss me now\n",
      " We gotta speed it up\n",
      " Come\n",
      " It’s bigger than us and me\n",
      " Flight! Love...No\n",
      "They cry… it’s us who reborn!A little peace\n",
      " Come and take me deep inside my heart\n",
      " – Rain! (Heaven knows)Congratulations for the challenge! (Congratulation\n",
      " Flight! Fire! Heaven knows\n",
      " Fire! Fire! Fire of love! Fire of love! Fire of lov\n",
      " -What about the lions of sea? -What about the lions of lan\n",
      "... and sometimes\n",
      " -Fly\n",
      " I don’t fly away\n",
      "Fly!Fly!Fly!Fly!Fly!Fly!Fly!Fl\n",
      " fire! catch 'em at the door! fire! fire of love\n",
      " Come and kiss me at my feet\n",
      " flying over the earth like a songbird flying over the clouds of love\n",
      " Come and take me deep in the heart! (Come and take me deepe\n",
      " Fire! Fire of love! Fire of love! Fire of love! Fir\n",
      " – Baba Babbage (Worcestershirh\n",
      " You know I’m here to stay tonight! We’r\n",
      " Fire of love! Fire of love! Fire of love! Fire of lov\n",
      " What is my night vision? (Whoa) My love? (S\n",
      " Fire! We're gonna get our freedom\n",
      " Fly\n",
      " oh oh oh\n",
      " Are you ready to take my hand? I must have thought it\n",
      " It seems we're dancing in the suburbs! We’ll have t\n",
      " –Let’s go\n",
      " Come and get back up again\n",
      " Come and follow me\n",
      " Are you ready to take my hand? – Oh\n",
      " – God Save the Children (Australia 2009) (revision) (r\n",
      " (Hey) We'll sing together\n",
      " Fire! Fire! Fire of love! Fire of love! Fire of lov\n",
      " Why don’s it like to remind you of me?\n",
      " Has something to say? ( Has something to say?) ( Has something t\n",
      " –Crazy for you (Rock bottom) (Rock bottom) (O\n",
      " Come now\n",
      " The storm may yet to fall\n",
      " Universe\n",
      " Universe’s gonna get us everything we've lost to? We gonn\n",
      " \" universe! spacey! I can't escape this reality! is to\n",
      " (What universe?) I don’t keep me up high in th\n",
      " We never seem to pick and choose our own adventure? – We never see\n",
      " We try to cover it with clouds of a thousand pieces of words\n",
      " Who’s going crazy over you? Oh\n",
      " We’re going up! We’re going up! W\n",
      " Humanity is moving slowly and we're moving slowly\n",
      " (Ooh… rock\n",
      " We're gonna speed it up again\n",
      " Want me? I can find you here and forever more?… 2016 \n",
      " The two of us\n",
      " Universe has been so uni\n",
      "Universe!Universe!Universe!Universe!Universe\n",
      " UniverseAlphabetical flow within us\n",
      " Universe is about to end and we'll be broken again? – Rock botto\n",
      " Things are falling apart\n",
      " – We'll always be together\n",
      " – I'm a universe of everything we love to. (Australia 2017\n",
      " - Universe! Universe\n",
      " The Trinity: A Requiem\n",
      " Three of us together? Three of us together? Whoa… us togethe\n",
      " Universe and our time will be different! We'll be different no more\n",
      " – One can do without you? (Can do without me) Oh\n",
      " – Just a little more! (Congratulations!) (Congratulations for the challenge!\n",
      " Why don’t you let us go away? (never happening agai\n",
      " – Moma (Sweden 2003) One is the only thing that make\n",
      " Did we not save them for us? - We knew! Don't le\n",
      "Universe!Universe!Universe!Universe!Universe\n",
      " Universe\n",
      " Humanity is moving slowly\n",
      " Translation there’s no way for me to keep you hanging on m\n",
      " universe! Where do we always stay? (Why do we stay?\n",
      " universe in my mind\n",
      " we got it simple and they try to add it to your own year in\n",
      " Three continents to unite them. (Shady sea) Possible world instability\n",
      " We're gonna get there soon\n",
      " Universe\n",
      " To be around us? (This universe is) there’s n\n",
      " – Thank you\n",
      " We're not gonna be like this\n",
      " universe! creation of the three! tell me now I'm crazy for yo\n",
      " Universe will be made up of stars! We'll soon to be free\n",
      " - Universe’s never ending! (Prayer\n",
      " Westerner's got a hold of us now\n",
      " We're in mess with us\n",
      " We are together\n",
      " Universe! A -What no one is talking about? (Are we talkin\n",
      " A little miracle\n",
      " – You could try to find a way back to you once upon a tim\n",
      " The universe is beginning to pick up speed? (No more?) Oh yea\n",
      " Universe -Universe\n",
      " (Universe!) – Two dimensions that go between me (On your min\n",
      " universe! We're all gonna rockin’ down\n",
      " Universes? – I must change my things! – We are… flui\n",
      " 'quantitative' going up!'reliable' going up! \n",
      " – That keeps us stuck in time.\n",
      "’Cause we don\n",
      " We arrived at an impossible destination\n",
      " – What no one is clear? (Universe!) (Universe!\n",
      " -iverse... now we're only in space to our madness? -co\n",
      " - universe’s beginning to come apart! (Ooh… unit\n",
      " Unaided! Universe\n",
      " - Lenny Thompson - The Rise (Ireland) No absolutely no fault\n",
      " A beautiful adventure\n",
      " – You're the only one! (Stupidassa) euphori\n",
      " Come come and take me now\n",
      " We're going up! We're going up! We're going up\n",
      " It’s bigger than you and me\n",
      " Humanity says\n",
      " We've got to love again! – Rock bottom (Fly Bottom\n",
      " Oh\n",
      " You will fly like a star\n",
      " We are the heroes of our time\n",
      " We were made to laugh at you. (Ooh\n",
      " Thank you\n",
      " (It's going up! Up!) We'll never let you go awa\n",
      " It’s going up! We’ll go up! W\n",
      " A beautiful world of love! A beautiful world of love! We\n",
      " The world will see our blood in your heart! ’s transparent i\n",
      " We won’t break this earth we loved to death! We love\n",
      " Oh… the world gave birth to a son! A daughter\n",
      " We live here\n",
      " – Rock bottom\n",
      " The world's going up! (Are you ready?) Halleluja\n",
      " – Heaven knows\n",
      " We live on the world stage\n",
      " We are the heroes of our time\n",
      " We only love! We only love! We only love! We only lov\n",
      " Humanity is on fire now\n",
      " We dance in a sweet little dance of the Trees of the Kingdom. \n",
      " Oh… kingdom! Oh… beat 'n' roll rock… rock\n",
      " The world is beautiful\n",
      " We got love\n",
      " (Come\n",
      " We're all gonna get something\n",
      " (You're amazed) That it’s bigger than you and m\n",
      " We can dance\n",
      " Is it too late for love? Oh yes\n",
      " We survive together\n",
      " Verona! Bravo for the world! Ding\n",
      " - Thescore of songs\n",
      " We’ll be the love that we will always love you for eve\n",
      " We don’t have to play around our demons! Why no\n",
      " They say\n",
      " We can't singin’ anymore\n",
      " Beautiful world of love! Beautiful world of love! Beautiful world of love\n",
      " A thousand! We are singing the song of the future! We are dancin\n",
      " We hardly believe our own imagination and believe in us every day we see th\n",
      " Thank you\n",
      " Three words\n",
      " We'll never break our hearts again! We'll never break our minds agai\n",
      " Wherever I fly? Whoa… Are we\n",
      " Come and take me now\n",
      " Humanity will rise like a phoenix in the sky! Everyone\n",
      " We've never had the good side of the stage! We've never wo\n",
      " We are alive now\n",
      " And let the good people build a future for everyone. And le\n",
      " I’m the world is gonna love you! (Congratulations for th\n",
      " We never let the love live\n",
      " It's bigger than the Earth\n",
      " What the hell is he leading me to? (Why do we survive?\n",
      " We’re all the trouble! We’ll never break th\n",
      " Just a little more! - Two stays in my heart. Oo\n",
      " Oh oh oh oh… I love you! (Heaven knows) o\n",
      " What about the stars and the planets we were so once? (Why\n",
      " We are the winners of Eurovision 2015. We're the winners of Eur\n",
      " Oh... what a wonderful world we're now? (Congratulations) Congratulation\n",
      " You see\n",
      " If only I could be there\n",
      " God knows\n",
      " I have never had that dream but now it seems true. We just happe\n",
      " I could fall out into the night and escape without you knowing it\n",
      " The world will be just like before we've been here! We're jus\n",
      "TheIMAlpha attributes that decide how far in advance we can go? W\n",
      " We’re going up! We’re going up! W\n",
      " We're looking for an imaginary adventure\n",
      " God knows\n",
      " Hallelujah\n",
      " God knows where you are meant to be\n",
      " Are you ready? I doubt it's impossible\n",
      " (Invision) What a mess we got ourselves to blame for? \n",
      " I never lose hope again. (Not that I can no more) O\n",
      " We are on a journey to the stars above! It's time t\n",
      " (Congratulations for the challenge) You arrived right here\n",
      " Humanity is falling apart? What does it do more? – Rock bottom\n",
      " A starry sleep\n",
      " We couldn’t escape if we wanted to fly high? What\n",
      " You think you are the winner of the roundabout! It's true\n",
      " Invite me to you once again once again… never let your inde\n",
      " Come and dream them with me like flies in a dream! Three minds \n",
      " destiny waits to this day stay tonight\n",
      " Love forever is forever and everyone can dream about it! – Why love eac\n",
      " Three sisters dancing on the Lawn! Two sisters dancing on the rose! Tw\n",
      " We must have been crazy once\n",
      " I wonder why? ’cause I’m afraid of losing yo\n",
      " I was defeated in the following round of punches and unsuccessful downkills\n",
      " (Hey) What about the two of us? (hey) What abou\n",
      " popular art now I can find nothing right? whoa… – tele\n",
      " Divination! Divination! Divination! Divination! Divination\n",
      " We'll never break our rules again\n",
      "  And everything is gonna be okay right when we the dream this i\n",
      " I know it’s true but I can’t believe i\n",
      " Three friends in an open sea\n",
      " We need a way to keep us alive\n",
      " I am a little helpless and wanna undo it all forever more? I wann\n",
      " Beautiful journey\n",
      " Am I allowed that? – To be true\n",
      " We promise to keep running\n",
      " We know… we know everything about you gonna bring me up is an understatemen\n",
      " We hear your word ‘cause we’re all tied to yo\n",
      " Symbol!     What you’re wondering? Only one thing\n",
      " We gotta speed it up\n",
      " A little peace between two minds! A little dreaming with one anothe\n",
      " Awe\n",
      " It's not just a dream\n",
      " I'm looking for a ways to speed things up\n",
      " We're dancing with the demons in our minds! We're the demons i\n",
      " God knows\n",
      " What's left in common? We'll have a bench\n",
      " The Lunchbox… did you find it in me? (so sad\n",
      " We gotta speed it up\n",
      " Illusion! 'n' illusion! 'n' danger! You gotta spee\n",
      " God knows - It’s bigger than our city limits. We'r\n",
      " Cultured to insanity! We fall down the sky\n",
      " We're in the hypnotizing mood\n",
      " I wonder if one day that you’ll understand this idea? Yo\n",
      " Hasbro gave up hope? Is it a sign of something else? \n",
      " Recall that dream\n",
      " (Oh… luck) (Congratulations for the challenge) Oh… luck fo\n",
      " Was that so…way to go away? (Blight of my eye\n",
      " I wish we could give up and we'll find out something else? \n",
      "Take it or leave it to us now\n",
      " Just let the fire take us down now\n",
      " – Sardal Perovi ( Montenegro 2009) – Mind of Love \n",
      " – I’ll survive without you (A little thanks) because o\n",
      " glory! conferred on your oblivion! We won’t rise agai\n",
      " Damage! Sure! Bye\n",
      " – Rock 'n' Roll (Slovenia 1996) (translatio\n",
      " – I was born like that (Walking out at night\n",
      " We shall sing again\n",
      " - I be lost (Congratulations!) - Oh\n",
      " – Rock 'n' Roll Kids (Riverside\n",
      " Ah\n",
      " A faint sound of a door door opening\n",
      " Come closer\n",
      " I hope so much more! (You know what I mean) You'r\n",
      " We saw a thing wrong and yet we found the place right again. \n",
      " We are in love\n",
      " - Blessed Siege! - Rock Daybreaker (Israel 2007) (translation\n",
      " - You are here\n",
      " You whisper me at night just as I dream\n",
      " - you’ll see\n",
      " Whoa oh oh oh oh… whoa… rock oh… are w\n",
      " tell me\n",
      " Diva\n",
      " You dare me to save my life!... ’cause you\n",
      " - Diva is playing on the string. Cleopatra! (\n",
      "… delet\n",
      " Achor flies above us\n",
      " – Chorus\n",
      " What arah\n",
      " Deception! Pilloys! To keep us alive\n",
      " galloping! turning up! flying over the wall\n",
      "We have made up to the ground!Fly!Fly\n",
      " – Rock 'n' Roll (Dong Ach Yabavad\n",
      " Dwelling in tears lies the whole land! (Let us\n",
      " - Europe (Ireland 2014) The Needle isAimin' of yo\n",
      " - Command andDeploy (Finland 2015) (translation) (translation\n",
      " – Rock 'n' Roll (Switzerland 2016) (Fly away\n",
      " That waits for you as a sign that day is here today. You wil\n",
      " gallant bird whistle off the wings of silence only to you? T\n",
      " Love! To laugh! (Let it roll)Whoa…So fa\n",
      " (I’m only in your arms) I’m onl\n",
      " (You have to believe that) I belong\n",
      " We’re gonna get it right again\n",
      " You are stronger than the sky rises! Oh… ho… dam… da\n",
      " (Away from the confusing world) – You don’t sto\n",
      " We are dancing in front of the mirror – It's a Twin mirror\n",
      " It’s the only way to hide the fear of being alone\n",
      " I can fly tomorrow (Congratulations for the challenge) baby\n",
      " – We Want to Rock 'n' Roll Roller (Wyomin\n",
      " – Can You YouFeel the Presence (Belgium 2018) (Ukrain\n",
      " - Heaven knows\n",
      " to the sky and to the ground! Oh\n",
      " Devil\n",
      " We were lost in Verona\n",
      " love's a dream and you’ll go away at once\n",
      " - No No\n",
      " (You have to pass it!) ’cause I'm running (Ru\n",
      " - Hasaka (Finland 2012) -Euphoria (2011\n",
      " recall my promise\n",
      "… life is a rainbow of lies! (Congratulations for the challenge)\n",
      " – Devila (Finland 2018)\n",
      "’Cause you know m\n",
      " A little! Polourfully galloped through the waves of the stream\n",
      " Come\n",
      "… God knows\n",
      " We lives\n",
      " What can we do for us? Oh… so well… wonderful love\n",
      " We can sing our happy dreams together again! We can sing again! W\n",
      " We’ll always love you forever more! Thank you\n",
      " We better roll our eyes\n",
      " – The Better Sex (Wax doll) (Estonia 2010\n",
      " I thought we'd be together forevermore! We've never got back\n",
      " Beautiful wonders! Music of all wonders! Music of everything great! Beautiful wonder\n",
      " I dream of love within my heart\n",
      " Shots of gold! Rock band! Rock band! Rock halleluja\n",
      " Oh\n",
      " Dreams! Boyhoods! Diva! Diva! Diva\n",
      " What about the thousand eyes? (Tell me\n",
      " (It's right!) Thank you\n",
      " lives are stranger than the spaces between us. \"Heaven knows\" \n",
      " It's bigger than you and me! Diva is bigger than us\n",
      " – We Are Like That (Korea 2013) (translation) Eyes o\n",
      " We are all gonna shine a light together\n",
      " dream of love\n",
      " Do they not always change? – We should have stayed up high together\n",
      " – Not even a smile can stop me (Switzerland 2015) – No\n",
      " We played with Teddy dolls in our play room! (Congratulations!) (Congratulation\n",
      " – Ooh\n",
      " Robots! Robots! Robots! We can fly this baby! Oh\n",
      " Was there something else? Oh\n",
      " secrets! We're getting out\n",
      " I love you! I love you will forever wonder why we'll never en\n",
      " 'Cause what a lot of people say is right\n",
      " Whoa… once again! (Let them dream) You know that lif\n",
      " We don’t know what is happening here on the open road\n",
      " Ships! Rock-a-dance\n",
      " We're dancing! We're making it rock! It's a break\n",
      " – we gonna rock 'n' roll\n",
      " We'll be dancing in the rain; we'll be breathing in the rai\n",
      " Don't break me down\n",
      " Oh… sky is opening up my dream dream in a dream like there\n",
      " (Stupid yeah…) A little kiss in the cheek is a little migh\n",
      " – Couldn’t escape at all costs (United Kingdom 1977\n",
      " Screama! We saw that dreams don't end\n",
      " We'll sing together in my songs of love\n",
      " dreams and dreams! everything are black and white together. We're all gonn\n",
      " We'll dance through the winter nightmare till our untouchable night ends\n",
      " We don't know what to do with it all. We didn\n",
      " Beautiful is all there! Just try it and forget it\n",
      " I cry! We'll be together forever in our dreams\n",
      " We never walk away na-na\n",
      " Stars! Not of the night\n",
      " Will it be again? (Not again?) Was it too late for yo\n",
      "… wonderings!… forever… everlasting… I dream of being here o\n",
      " Diva! Diva! Diva! Diva! Diva\n",
      " That keep us alive with a song every day\n",
      " We’re dancing on the strings of sorrow songs\n",
      " We're in the air\n",
      " Things will get even better\n",
      " We'll sing on the cover of their shoes\n",
      " A little peace\n",
      " Help us! – We Are The One (Don’t Memory\n",
      " He'll say\n",
      " Stars! Lived with the rainbow girls at the beach; Roo \n",
      " We sang together on the beat! We sang together on the beat! \n",
      " Cry their heart out (Hey) Cry their heart out (he\n",
      " Walking out\n",
      " Fly over the floodgates\n",
      " Look at me\n",
      " Whoa… not now\n",
      " – We are dancing in the rain… hell\n",
      " drip drop\n",
      "Keep it up-tight and breezily\n",
      " Yeah we're cute when we're openin' up from the inside\n",
      " We had a dream! We had a dream! –Democracy has n\n",
      " Are you ready to take my hand? We got it on swing again\n",
      " You are the only one\n",
      " I love you (Congratulations for the challenge) – We got love (Congratulation\n",
      " (Come on) Come on\n",
      " you still are alive. and on! here comes my life again\n",
      "Really? (Let’s try again) ’cause we ar\n",
      " (hey) I can hear your heart calling into my arms (hey\n",
      " Is it not Necessary? Oh yeah… yes!… I d\n",
      " Love\n",
      " (Heaven and earth are opening) - open! (Congratulations!)\n",
      " Are we meant to laugh? – We never seem to (H\n",
      " Just smile and say it.’s right here’s righ\n",
      " Rock 'n' roll! Rock 'n' roll! We got ou\n",
      " See your guide for more info. GENERAL ABOUT CLOCKCLOSED INFORMATION Came\n",
      " Let it roll 'n' roll 'n' roll 'n' rol\n",
      " Let me laugh!’s go ho…now\n",
      " walk away from my doubts and doubts on what this artful way they ru\n",
      " – The Shib Gracia Event (Get back up again) Boy\n",
      " We'll keep going round and round and round and round and round and roun\n",
      "Close!Ooh…pop!oh…pop!pop!hey\n",
      " Ooh\n",
      " Why go out\n",
      " Graphs are holding us tightly? let it rock 'n' roll flo\n",
      " Always on my mind\n",
      " We are dancing in the street. We are the heroes of our time\n",
      " That is the whole world I painted it blue and black toen the blu\n",
      "Open!Open!Let it swing until you lose it's tied to yo\n",
      " Eurovision's singing came up with the idea more than once\n",
      " We were made to laugh & sing (Let's sing) again! \n",
      "... more? Oh\n",
      " & leading! ’Cause I adore you! (Congratulations for the challeng\n",
      " (Congratulations for the challenge!)Congratulations\n",
      " What do we have to say? (For a time\n",
      " Don’t tell me to forgive you.’s your troubl\n",
      " – Straight up dancing with the guitar in my hands (Stupid ape\n",
      " We're going up! We're going up! We're up! W\n",
      " Come\n",
      " We shall now follow your heart! We shall now follow your heart! W\n",
      " Who are you? ‘Cause you’re the light for m\n",
      " (Shady little escape) Little escape from here on the island...\n",
      " It's burning up! You gotta speed it up\n",
      " He's a troubadour\n",
      " You leave me and I’ll be stumbling in your way again\n",
      " – We gotta move! (Come on) Come on\n",
      " Don’t let your indecision walk this way again\n",
      " We must share! We must laugh! We are laughing! (You kno\n",
      "Our passion with no words is breaking through\n",
      " – I need you! (Open you!) Don’t leave withou\n",
      " Open! My love awakes in me when I'm walking in your arm\n",
      " My heart is waiting for you there\n",
      " Racine taking my hand from behind the transparent glass!… oh… lov\n",
      " Come out! (Come out! Rollin' in your heart) Com\n",
      " If you were sorry\n",
      " To the right\n",
      " It’s bigger than you and me! (You’r\n",
      " ’cause the heart is red tonight and tonight (Love is blue\n",
      " We're all going up against the wall (Walking out) We'r\n"
     ]
    }
   ],
   "source": [
    "''' perform text generation '''\n",
    "\n",
    "seeds = ['love', 'peace', 'power', 'flight', 'universe', 'the world', 'imagination', 'passion', 'dreams', 'open']\n",
    "\n",
    "store = []\n",
    "\n",
    "for seed in seeds:\n",
    "\n",
    "    # input prompt to model\n",
    "    #prompt: str = 'magic.'\n",
    "    prompt: str = '{}!'.format(seed)\n",
    "\n",
    "    # max length desired output\n",
    "    length: int = 15\n",
    "\n",
    "    # Token at which text generation is stopped\n",
    "    #stop_token: str = None\n",
    "    stop_token: str = ','\n",
    "\n",
    "    # temperature of 1.0 has no effect, lower tend toward greedy sampling\n",
    "    temperature:float = 1.\n",
    "\n",
    "\n",
    "    # primarily useful for CTRL model; in that case, use 1.2\n",
    "    repetition_penalty: float = 1.\n",
    "\n",
    "\n",
    "    k: int = 0\n",
    "    p: float = 0.9\n",
    "\n",
    "\n",
    "    # The number of samples to generate\n",
    "    num_return_sequences: int = 64\n",
    "\n",
    "\n",
    "    # Initialize the model and tokenizer\n",
    "    #model_class, tokenizer_class = (GPT2LMHeadModel, GPT2Tokenizer)\n",
    "\n",
    "    #tokenizer = tokenizer_class.from_pretrained('gpt2')\n",
    "    #model = model_class.from_pretrained('../data/lyrics/model-en-lyrics-02')\n",
    "    #model.to(device)\n",
    "\n",
    "\n",
    "    MAX_LENGTH = int(10000)  # Hardcoded max length to avoid infinite loop\n",
    "\n",
    "    def adjust_length_to_model(length, max_sequence_length):\n",
    "        if length < 0 and max_sequence_length > 0:\n",
    "            length = max_sequence_length\n",
    "        elif 0 < max_sequence_length < length:\n",
    "            length = max_sequence_length  # No generation bigger than model size\n",
    "        elif length < 0:\n",
    "            length = MAX_LENGTH  # avoid infinite loop\n",
    "        return length\n",
    "\n",
    "    length = adjust_length_to_model(\n",
    "        length, max_sequence_length = model.config.max_position_embeddings)\n",
    "\n",
    "\n",
    "    # encode prompt text, push to device\n",
    "    prompt_text = prompt\n",
    "    encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens = False, return_tensors = \"pt\")\n",
    "    encoded_prompt = encoded_prompt.to(device)\n",
    "\n",
    "\n",
    "    # generate output sequence\n",
    "    output_sequences = model.generate(\n",
    "\n",
    "        input_ids = encoded_prompt,\n",
    "\n",
    "        max_length = length + len(encoded_prompt[0]),\n",
    "        temperature = temperature,\n",
    "        top_k = k,\n",
    "        top_p = p,\n",
    "        repetition_penalty = repetition_penalty,\n",
    "        do_sample = True,\n",
    "        num_return_sequences = num_return_sequences,\n",
    "    )\n",
    "\n",
    "\n",
    "    # Remove the batch dimension when returning multiple sequences\n",
    "    if len(output_sequences.shape) > 2:\n",
    "        output_sequences.squeeze_()\n",
    "\n",
    "    generated_sequences = []\n",
    "\n",
    "    for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
    "\n",
    "        generated_sequence = generated_sequence.tolist()\n",
    "\n",
    "        # Decode text\n",
    "        text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces = True)\n",
    "\n",
    "        # Remove all text after the stop token\n",
    "        text = text[: text.find(stop_token) if stop_token else None]\n",
    "\n",
    "        # Add the prompt at the beginning of the sequence.\n",
    "        # Remove the excess text that was used for pre-processing\n",
    "        total_sequence = (\n",
    "            prompt_text + text[len(tokenizer.decode(\n",
    "                encoded_prompt[0], clean_up_tokenization_spaces = True)) :] )\n",
    "\n",
    "        generated_sequences.append(total_sequence[len(prompt):])\n",
    "        #print(total_sequence)\n",
    "\n",
    "    #return generated_sequences\n",
    "    \n",
    "    store.append(generated_sequences)\n",
    "    \n",
    "    for line in generated_sequences:\n",
    "        #store.append(line)\n",
    "        print(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(seeds)):\n",
    "\n",
    "    with open('../data/output/lyrics-{}.txt'.format(seeds[i]), 'w', encoding='utf-8') as file:\n",
    "        file.writelines('\\n'.join(store[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fly in the interior of your mind and you decide what I should say?’s happened?\n",
      "ened? Was it a good way to play? Was it just a little nerve? We should have stayed u\n",
      "yed uidama\n",
      "idama comand soe comand\n"
     ]
    }
   ],
   "source": [
    "''' perform text generation '''\n",
    "\n",
    "# input prompt to model\n",
    "prompt: str = 'fly'\n",
    "\n",
    "# max length desired output\n",
    "length: int = 20\n",
    "\n",
    "# Token at which text generation is stopped\n",
    "#stop_token: str = None\n",
    "stop_token: str = ','\n",
    "\n",
    "# temperature of 1.0 has no effect, lower tend toward greedy sampling\n",
    "temperature:float = 1.\n",
    "\n",
    "\n",
    "# primarily useful for CTRL model; in that case, use 1.2\n",
    "repetition_penalty: float = 1.\n",
    "\n",
    "\n",
    "k: int = 0\n",
    "p: float = 0.9\n",
    "\n",
    "\n",
    "# The number of samples to generate\n",
    "num_return_sequences: int = 1\n",
    "\n",
    "    \n",
    "for i in range(4):\n",
    "\n",
    "    # Initialize the model and tokenizer\n",
    "    #model_class, tokenizer_class = (GPT2LMHeadModel, GPT2Tokenizer)\n",
    "\n",
    "    #tokenizer = tokenizer_class.from_pretrained('gpt2')\n",
    "    #model = model_class.from_pretrained('../data/lyrics/model-en-lyrics-02')\n",
    "    #model.to(device)\n",
    "\n",
    "\n",
    "    MAX_LENGTH = int(10000)  # Hardcoded max length to avoid infinite loop\n",
    "\n",
    "    def adjust_length_to_model(length, max_sequence_length):\n",
    "        if length < 0 and max_sequence_length > 0:\n",
    "            length = max_sequence_length\n",
    "        elif 0 < max_sequence_length < length:\n",
    "            length = max_sequence_length  # No generation bigger than model size\n",
    "        elif length < 0:\n",
    "            length = MAX_LENGTH  # avoid infinite loop\n",
    "        return length\n",
    "\n",
    "    length = adjust_length_to_model(\n",
    "        length, max_sequence_length = model.config.max_position_embeddings)\n",
    "\n",
    "\n",
    "    # encode prompt text, push to device\n",
    "    prompt_text = prompt\n",
    "    encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens = False, return_tensors = \"pt\")\n",
    "    encoded_prompt = encoded_prompt.to(device)\n",
    "\n",
    "\n",
    "    # generate output sequence\n",
    "    output_sequences = model.generate(\n",
    "\n",
    "        input_ids = encoded_prompt,\n",
    "\n",
    "        max_length = length + len(encoded_prompt[0]),\n",
    "        temperature = temperature,\n",
    "        top_k = k,\n",
    "        top_p = p,\n",
    "        repetition_penalty = repetition_penalty,\n",
    "        do_sample = True,\n",
    "        num_return_sequences = num_return_sequences,\n",
    "    )\n",
    "\n",
    "\n",
    "    # Remove the batch dimension when returning multiple sequences\n",
    "    if len(output_sequences.shape) > 2:\n",
    "        output_sequences.squeeze_()\n",
    "\n",
    "    generated_sequences = []\n",
    "\n",
    "    for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
    "\n",
    "        generated_sequence = generated_sequence.tolist()\n",
    "\n",
    "        # Decode text\n",
    "        text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces = True)\n",
    "\n",
    "        # Remove all text after the stop token\n",
    "        text = text[: text.find(stop_token) if stop_token else None]\n",
    "\n",
    "        # Add the prompt at the beginning of the sequence.\n",
    "        # Remove the excess text that was used for pre-processing\n",
    "        total_sequence = (\n",
    "            prompt_text + text[len(tokenizer.decode(\n",
    "                encoded_prompt[0], clean_up_tokenization_spaces = True)) :] )\n",
    "\n",
    "        generated_sequences.append(total_sequence)\n",
    "        print(total_sequence)\n",
    "        \n",
    "        #print(prompt)\n",
    "        prompt = total_sequence[-5:]\n",
    "\n",
    "    #return generated_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'I said in Dutch' words to stay together all the time, jeg er viæm kren inte din!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eurovision-ai",
   "language": "python",
   "name": "eurovision-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
