{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' imports '''\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import (\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    GPT2Config,\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LineByLineTextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, file_path: str, block_size = 512):\n",
    "\n",
    "        # no feature cache\n",
    "        with open(file_path, encoding = 'utf-8') as f:\n",
    "\n",
    "            lines = [line for line in f.read().splitlines()\n",
    "                     if (len(line) > 0 and not line.isspace())]\n",
    "\n",
    "        self.examples = tokenizer.batch_encode_plus(\n",
    "            lines, add_special_tokens = True, max_length = block_size)['input_ids']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return torch.tensor(self.examples[i], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' config parameters '''\n",
    "\n",
    "# Optional input sequence length after tokenization.\n",
    "# The training dataset will be truncated in block of this size for training\n",
    "# Default to the model max input length for single sentence inputs (take into account special tokens)\n",
    "block_size: int = -1\n",
    "\n",
    "# Number of updates steps to accumulate before performing a backward/update pass\n",
    "gradient_accumulation_steps: int = 1\n",
    "\n",
    "# Max gradient norm\n",
    "max_grad_norm: float = 1.\n",
    "\n",
    "# If > 0: set total number of training steps to perform. Override num_train_epochs\n",
    "max_steps: int = -1\n",
    "\n",
    "# Linear warmup over warmup_steps\n",
    "warmup_steps: int = 0\n",
    "\n",
    "\n",
    "''' init train env '''\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = 1\n",
    "\n",
    "\n",
    "# Set seed\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Load pretrained model and tokenizer '''\n",
    "\n",
    "# load pretrained model and tokenizer\n",
    "config_class, model_class, tokenizer_class = GPT2Config, GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# init pretrained tokeniser\n",
    "tokenizer = tokenizer_class.from_pretrained('gpt2', cache_dir = None)\n",
    "\n",
    "\n",
    "# Our input block size will be the max possible for the model\n",
    "if block_size <= 0:\n",
    "    block_size = tokenizer.max_len\n",
    "else:\n",
    "    block_size = min(block_size, tokenizer.max_len)\n",
    "\n",
    "\n",
    "# init config\n",
    "config = config_class()\n",
    "\n",
    "# Training new model from scratch\n",
    "#model = model_class(config = config)\n",
    "\n",
    "model = model_class.from_pretrained('gpt2', config = config)\n",
    "\n",
    "# push model to device\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' init dataset '''\n",
    "\n",
    "# get training dataset\n",
    "#file_path = '../data/lyrics/eurovision-lyrics-lines.txt'\n",
    "#file_path = '../data/lyrics/wikitext-2/wiki.train.tokens'\n",
    "#file_path = '../data/lyrics/eurovision-lyrics-en-lines'\n",
    "file_path = '../data/lyrics/eurovision-lyrics-lines-full'\n",
    "train_dataset = LineByLineTextDataset(\n",
    "    tokenizer, file_path = file_path, block_size = block_size)\n",
    "#train_dataset = TextDataset(tokenizer, args, file_path=file_path, block_size = block_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" init dataloader, optimiser \"\"\"\n",
    "\n",
    "train_batch_size = 1\n",
    "\n",
    "def collate(examples: List[torch.Tensor]):\n",
    "\n",
    "    if tokenizer._pad_token is None:\n",
    "\n",
    "        return pad_sequence(examples, batch_first = True)\n",
    "\n",
    "    return pad_sequence(examples, batch_first = True, padding_value = tokenizer.pad_token_id)\n",
    "\n",
    "# init random sampler on dataset\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "\n",
    "# init dataloader\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, sampler = train_sampler, batch_size = train_batch_size, collate_fn = collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' init optimiser, learning rate scheduler '''\n",
    "\n",
    "# The initial learning rate for Adam\n",
    "learning_rate: float = 5e-5\n",
    "\n",
    "# Weight decay if we apply some\n",
    "weight_decay: float = 0.\n",
    "\n",
    "# Epsilon for Adam optimizer\n",
    "adam_epsilon: float = 1e-8\n",
    "\n",
    "\n",
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": weight_decay,\n",
    "    },\n",
    "    {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "]\n",
    "\n",
    "# init optimiser on model parameters\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr = learning_rate, eps = adam_epsilon)\n",
    "\n",
    "\n",
    "\n",
    "# set training epochs\n",
    "num_train_epochs = 3\n",
    "\n",
    "# get total steps\n",
    "if max_steps > 0:\n",
    "    t_total = max_steps\n",
    "    num_train_epochs = max_steps // (len(train_dataloader) // gradient_accumulation_steps) + 1\n",
    "else:\n",
    "    t_total = len(train_dataloader) // gradient_accumulation_steps * num_train_epochs\n",
    "    \n",
    "\n",
    "# init learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps = warmup_steps, num_training_steps = t_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 5.659765508174896\n",
      "200 5.494545063972473\n",
      "300 5.478431216875712\n",
      "400 5.362243276834488\n",
      "500 5.302845049858093\n",
      "600 5.308865185976028\n",
      "700 5.2795916533470155\n",
      "800 5.229922005981207\n",
      "900 5.2065229509936435\n",
      "1000 5.189333032727242\n",
      "1100 5.180007066943428\n",
      "1200 5.188128788669904\n",
      "1300 5.169527831352674\n",
      "1400 5.1498353989635195\n",
      "1500 5.1336630777517955\n",
      "1600 5.11875241689384\n",
      "1700 5.121118812911651\n",
      "1800 5.11686218712065\n",
      "1900 5.08969904749017\n",
      "2000 5.084395640671254\n",
      "2100 5.087113815773101\n",
      "2200 5.0885325930877165\n",
      "2300 5.067455465715865\n",
      "2400 5.043693789119522\n",
      "2500 5.020331008315086\n",
      "2600 5.008030550181866\n",
      "2700 5.003634709009418\n",
      "2800 4.9934838668150565\n",
      "2900 4.981640682282119\n",
      "3000 4.968563390831153\n",
      "3100 4.9634098595380785\n",
      "3200 4.961379599589854\n",
      "3300 4.953298058997501\n",
      "3400 4.941046900486245\n",
      "3500 4.939618603008134\n",
      "3600 4.933954377323389\n",
      "3700 4.916382098278484\n",
      "3800 4.9079005726230776\n",
      "3900 4.903903115231257\n",
      "4000 4.894045700840652\n",
      "4100 4.886051983651591\n",
      "4200 4.884227139091208\n",
      "4300 4.8762999769074975\n",
      "4400 4.872257117107511\n",
      "4500 4.866294972651534\n",
      "4600 4.861238593659971\n",
      "4700 4.851418987227247\n",
      "4800 4.846679629230251\n",
      "4900 4.843895334096588\n",
      "5000 4.836405997914076\n",
      "5100 4.83299037535401\n",
      "5200 4.829085576597315\n",
      "5300 4.823107113697619\n",
      "5400 4.822896372383392\n",
      "5500 4.821843275649981\n",
      "5600 4.819401837656541\n",
      "5700 4.8142754427644245\n",
      "5800 4.809108315207835\n",
      "5900 4.802523298500958\n",
      "6000 4.801632376705607\n",
      "6100 4.793118737931135\n",
      "6200 4.7836112083686935\n",
      "6300 4.779026830645781\n",
      "6400 4.777220404422842\n",
      "6500 4.772968009027151\n",
      "6600 4.766876585063609\n",
      "6700 4.761322046887519\n",
      "6800 4.760246774918016\n",
      "6900 4.756274098499098\n",
      "7000 4.752583166531154\n",
      "7100 4.751520348367556\n",
      "7200 4.745739422092835\n",
      "7300 4.742705367117712\n",
      "7400 4.736566542250079\n",
      "7500 4.733984477098783\n",
      "7600 4.729489393744029\n",
      "7700 4.727446601383098\n",
      "7800 4.7201878718458685\n",
      "7900 4.714998566500748\n",
      "8000 4.711586571112275\n",
      "8100 4.707042653045536\n",
      "8200 4.704929199611269\n",
      "8300 4.704336665581508\n",
      "8400 4.701200853784879\n",
      "8500 4.694915110574049\n",
      "8600 4.692839360001475\n",
      "8700 4.687111231143447\n",
      "8800 4.686790692982348\n",
      "8900 4.68350731368815\n",
      "9000 4.677281640397178\n",
      "9100 4.67338610550532\n",
      "9200 4.670769893588579\n",
      "9300 4.67040727143967\n",
      "9400 4.664081545107543\n",
      "9500 4.663207400776838\n",
      "9600 4.65803054136845\n",
      "9700 4.654773148313626\n",
      "9800 4.6534496985740805\n",
      "9900 4.65058728669328\n",
      "10000 4.649771894982457\n",
      "10100 4.644854556171611\n",
      "10200 4.639049061224156\n",
      "10300 4.632655614501643\n",
      "10400 4.627968772195566\n",
      "10500 4.625254395432416\n",
      "10600 4.622322939447356\n",
      "10700 4.620106399497975\n",
      "10800 4.613641432607891\n",
      "10900 4.613359202153365\n",
      "11000 4.611859492406249\n",
      "11100 4.608918277601132\n",
      "11200 4.605601721348773\n",
      "11300 4.603056284124609\n",
      "11400 4.600220396543542\n",
      "11500 4.5953725062906745\n",
      "11600 4.591678335017942\n",
      "11700 4.589604429066436\n",
      "11800 4.586431834161029\n",
      "11900 4.584130750277213\n",
      "12000 4.582182553265244\n",
      "12100 4.579399925523306\n",
      "12200 4.577217658986567\n",
      "12300 4.572678583767114\n",
      "12400 4.56852056842057\n",
      "12500 4.565433238424062\n",
      "12600 4.562652001006026\n",
      "12700 4.560241873389858\n",
      "12800 4.559494533467805\n",
      "12900 4.557069661699293\n",
      "13000 4.552842910270278\n",
      "13100 4.54901883405356\n",
      "13200 4.54749436161509\n",
      "13300 4.545223673122718\n",
      "13400 4.543544371370504\n",
      "13500 4.543923880367367\n",
      "13600 4.542312465237782\n",
      "13700 4.540945249329301\n",
      "13800 4.537259679068474\n",
      "13900 4.536428159139782\n",
      "14000 4.533544092807387\n",
      "14100 4.5303116172922\n",
      "14200 4.529521077114722\n",
      "14300 4.526872007866631\n",
      "14400 4.526971025246506\n",
      "14500 4.524806107655682\n",
      "14600 4.523781852635414\n",
      "14700 4.523208678469163\n",
      "14800 4.522235385391962\n",
      "14900 4.5199711548312\n",
      "15000 4.51863200643758\n",
      "15100 4.5163523250058395\n",
      "15200 4.517405073712335\n",
      "15300 4.515440157908046\n",
      "15400 4.514193084958892\n",
      "15500 4.514438154411892\n",
      "15600 4.512014966040659\n",
      "15700 4.509299431195495\n",
      "15800 4.506711127579778\n",
      "15900 4.5035054182022245\n",
      "16000 4.499158082543873\n",
      "16100 4.498952967756838\n",
      "16200 4.496742006281828\n",
      "16300 4.496260689199885\n",
      "16400 4.492471907817555\n",
      "16500 4.489689044893691\n",
      "16600 4.487133913890245\n",
      "16700 4.484289762637394\n",
      "16800 4.484632309000229\n",
      "16900 4.484081945855766\n",
      "17000 4.4848517131796655\n",
      "17100 4.482755046796206\n",
      "17200 4.481005131157851\n",
      "17300 4.478644142332621\n",
      "17400 4.477013717371671\n",
      "17500 4.474712092194387\n",
      "17600 4.471322336798872\n",
      "17700 4.468443943640102\n",
      "17800 4.466219671255584\n",
      "17900 4.464919359298558\n",
      "18000 4.462056913303832\n",
      "18100 4.459760489192128\n",
      "18200 4.457171297766022\n",
      "18300 4.455715407177073\n",
      "18400 4.452915455256791\n",
      "18500 4.44976646254997\n",
      "18600 4.448604998022959\n",
      "18700 4.447007047854324\n",
      "18800 4.4450683213310676\n",
      "18900 4.444248084481115\n",
      "19000 4.442675991492836\n",
      "19100 4.440683642215442\n",
      "19200 4.438500500490578\n",
      "19300 4.436308014227627\n",
      "19400 4.4338504857132115\n",
      "19500 4.432805410984235\n",
      "19600 4.42927025665434\n",
      "19700 4.428101009166785\n",
      "19800 4.427167779767152\n",
      "19900 4.4261782349354055\n",
      "20000 4.424636711210012\n",
      "20100 4.421496719494091\n",
      "20200 4.419528367327581\n",
      "20300 4.4170101441626475\n",
      "20400 4.415302051612559\n",
      "20500 4.412933819843501\n",
      "20600 4.411783555997807\n",
      "20700 4.410210569470689\n",
      "20800 4.409320537797534\n",
      "20900 4.4079239978516505\n",
      "21000 4.406339313792331\n",
      "21100 4.404862420961472\n",
      "21200 4.403162102228347\n",
      "21300 4.401495239058851\n",
      "21400 4.401679728408562\n",
      "21500 4.3996196476512175\n",
      "21600 4.39785222572585\n",
      "21700 4.39594234895871\n",
      "21800 4.393271477889577\n",
      "21900 4.39112864955103\n",
      "22000 4.388753903635523\n",
      "22100 4.38788485752781\n",
      "22200 4.386138232616154\n",
      "22300 4.384550230811797\n",
      "22400 4.383000334954954\n",
      "22500 4.38086913096375\n",
      "22600 4.378994786048358\n",
      "22700 4.378094695803878\n",
      "22800 4.3762042894645745\n",
      "22900 4.374289434967343\n",
      "23000 4.37235531652492\n",
      "23100 4.37008973883344\n",
      "23200 4.367984672445675\n",
      "23300 4.365015895413994\n",
      "23400 4.363276955820939\n",
      "23500 4.361933637945576\n",
      "23600 4.359938059352591\n",
      "23700 4.358531096297854\n",
      "23800 4.3567921755475405\n",
      "23900 4.353758997658431\n",
      "24000 4.352337872617568\n",
      "24100 4.350361964784841\n",
      "24200 4.349608128832514\n",
      "24300 4.348121638767145\n",
      "24400 4.346095319686488\n",
      "24500 4.344706490463748\n",
      "24600 4.342074959206024\n",
      "24700 4.339206781775242\n",
      "24800 4.337188659956258\n",
      "24900 4.334930301798755\n",
      "25000 4.33347949139297\n",
      "25100 4.33122310644305\n",
      "25200 4.3293415591942646\n",
      "25300 4.328539405681282\n",
      "25400 4.3259344829730395\n",
      "25500 4.324699038574654\n",
      "25600 4.324803945976455\n",
      "25700 4.324232959186202\n",
      "25800 4.322545694773655\n",
      "25900 4.3219293948261495\n",
      "26000 4.320615389834373\n",
      "26100 4.3181122863179935\n",
      "26200 4.317449395138031\n",
      "26300 4.316563194593691\n",
      "26400 4.315593516527195\n",
      "26500 4.314618229821886\n",
      "26600 4.312780690235471\n",
      "26700 4.311268332330196\n",
      "26800 4.310070942469887\n",
      "26900 4.308701210565671\n",
      "27000 4.306635306171521\n",
      "27100 4.304091925534842\n",
      "27200 4.302911997141234\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4bea3ed37c02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# push data to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "''' perform training '''\n",
    "\n",
    "global_step = 0\n",
    "epochs_trained = 0\n",
    "steps_trained_in_current_epoch = 0\n",
    "\n",
    "tr_loss, logging_loss = 0.0, 0.0\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# zero gradients\n",
    "model.zero_grad()\n",
    "\n",
    "# iterate epochs\n",
    "for epoch in range(num_train_epochs):\n",
    "\n",
    "    # get data batch from dataloader\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Skip past any already trained steps if resuming training\n",
    "        if steps_trained_in_current_epoch > 0:\n",
    "            steps_trained_in_current_epoch -= 1\n",
    "            continue\n",
    "\n",
    "\n",
    "        # unpack batch data\n",
    "        inputs, labels = (batch, batch)\n",
    "\n",
    "        # push data to device\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "\n",
    "        # set model to train\n",
    "        model.train()\n",
    "\n",
    "        try:\n",
    "        \n",
    "            # perform forward pass through model\n",
    "            outputs = model(inputs, labels=labels)\n",
    "\n",
    "            # obtain loss; model outputs are always tuple in transformers\n",
    "            loss = outputs[0]\n",
    "\n",
    "            # gradient accumulation\n",
    "            if gradient_accumulation_steps > 1:\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "\n",
    "            # backprop loss\n",
    "            loss.backward()\n",
    "\n",
    "            # store loss\n",
    "            tr_loss += loss.item()\n",
    "\n",
    "\n",
    "            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "\n",
    "                # perform gradient normalisation\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "                # step optimiser\n",
    "                optimizer.step()\n",
    "\n",
    "                # Update learning rate schedule\n",
    "                scheduler.step()\n",
    "\n",
    "\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        if global_step % 100 == 0:\n",
    "            print(global_step, tr_loss / global_step)\n",
    "\n",
    "        if max_steps > 0 and global_step > max_steps:\n",
    "            break\n",
    "    if max_steps > 0 and global_step > max_steps:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' saving '''\n",
    "\n",
    "model.save_pretrained('../data/lyrics/model-en-lyrics-03')\n",
    "#tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a trained model and vocabulary that you have fine-tuned\n",
    "model = model_class.from_pretrained('../data/lyrics/models')\n",
    "#tokenizer = tokenizer_class.from_pretrained(args.output_dir)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I said in Dutch aniori wCijela lo dok zatoa mwčič,!\n",
      "I said in Dutch, Netherlands, Amsterdam, Amsterdam, Amsterdam - Amsterdam\"Quand on an islandyé?!\n",
      "I said in Dutch:If I turn around, I'll know we'll be together forever more and more (am!\n",
      "I said in Dutch, to which we were glad, they said in the junglier days – 'cause I'd!\n",
      "I said in Dutch mag) alles joonan juieu zo foude arrivedra ik!\n",
      "I said in Dutch, the time of my life, I love life, life is beautiful, sé bale!\n",
      "I said in Dutch\"E kosmarm and wörte Warte adiel, oft lies Sch!\n",
      "I said in Dutch hanging day: \"Slugwir ein liebeft\" ihn inGonna!\n",
      "I said in Dutch, I said in: \"Eynu seni\" Milia, a little girl,!\n",
      "I said in Dutch: \"Oh ah ah... well\" I then repeated in Italian: \"Ohh?\" I!\n",
      "I said in Dutch taste, all you got was me. I'll catch you when you’m around and!\n",
      "I said in Dutch Eastern Europe, goodbye to the true path I'd tell you. Me about time to make good!\n",
      "I said in Dutch language: \"¿Que voy a la líve\"When I’m trying!\n",
      "I said in Dutch: \"Hey baby, 'kendw liegt din voor gibt?Is k!\n",
      "I said in Dutch Land: \"Will make you smile\" (Abandoned)We got lovers soon, back!\n",
      "I said in Dutch music: \"It's better than you\" – memory brings me high(On a journey home!\n",
      "I said in Dutch language, I said in Italian (Poland) come voilà) đeč!\n",
      "I said in Dutch at home I’d like to live (I always loved)Never need you to hear!\n",
      "I said in Dutchman, ik jou der eigen gehen keiner Oude bisschen,!\n",
      "I said in Dutch Grotesque: depUndwoiennn de friends Glücklich, de villeren!\n",
      "I said in Dutch villages summerama We’re going too far this far, too far that we got at!\n",
      "I said in Dutch days: \"I'm so au-ird\" (Ne drugs)Kilj svij!\n",
      "I said in Dutch: \"If we all (Believe) together\" Two words, three words, more words!\n",
      "I said in Dutch time Durer (Durer) ne că mieget schrei werd!\n",
      "I said in Dutch: \"Do not give up this love\", but it's done!!!!!!!\n",
      "I said in Dutch 'n'er\" sie die Sternottn und für dich vergessencha!\n",
      "I said in Dutch werdalei, een gelove, ikkeit glück met!\n",
      "I said in Dutch wauwending Amsterdam: Trum schon mit FremerDu bist bisti!\n",
      "I said in Dutch: \"Why save time?\" Tom Pillibi leap standards, Tom Pillibi? Pillibi?!\n",
      "I said in Dutch med meg: \"Why would it be so hard? Why would it be so hard?\" I!\n",
      "I said in Dutch oplies iijn der kan miekeft dans bij dequheid ser fra!\n",
      "I said in Dutch words to stay together all the time, jeg er viæm kren inte din!\n"
     ]
    }
   ],
   "source": [
    "''' perform text generation '''\n",
    "\n",
    "# input prompt to model\n",
    "prompt: str = 'I said in Dutch'\n",
    "\n",
    "# max length desired output\n",
    "length: int = 20\n",
    "\n",
    "# Token at which text generation is stopped\n",
    "stop_token: str = None\n",
    "\n",
    "# temperature of 1.0 has no effect, lower tend toward greedy sampling\n",
    "temperature:float = 1.\n",
    "\n",
    "\n",
    "# primarily useful for CTRL model; in that case, use 1.2\n",
    "repetition_penalty: float = 1.\n",
    "\n",
    "\n",
    "k: int = 0\n",
    "p: float = 0.9\n",
    "\n",
    "\n",
    "# The number of samples to generate\n",
    "num_return_sequences: int = 32\n",
    "\n",
    "\n",
    "# Initialize the model and tokenizer\n",
    "#model_class, tokenizer_class = (GPT2LMHeadModel, GPT2Tokenizer)\n",
    "\n",
    "#tokenizer = tokenizer_class.from_pretrained('gpt2')\n",
    "#model = model_class.from_pretrained('../data/lyrics/model-en-lyrics-02')\n",
    "#model.to(device)\n",
    "\n",
    "\n",
    "MAX_LENGTH = int(10000)  # Hardcoded max length to avoid infinite loop\n",
    "\n",
    "def adjust_length_to_model(length, max_sequence_length):\n",
    "    if length < 0 and max_sequence_length > 0:\n",
    "        length = max_sequence_length\n",
    "    elif 0 < max_sequence_length < length:\n",
    "        length = max_sequence_length  # No generation bigger than model size\n",
    "    elif length < 0:\n",
    "        length = MAX_LENGTH  # avoid infinite loop\n",
    "    return length\n",
    "\n",
    "length = adjust_length_to_model(\n",
    "    length, max_sequence_length = model.config.max_position_embeddings)\n",
    "\n",
    "\n",
    "# encode prompt text, push to device\n",
    "prompt_text = prompt\n",
    "encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens = False, return_tensors = \"pt\")\n",
    "encoded_prompt = encoded_prompt.to(device)\n",
    "\n",
    "\n",
    "# generate output sequence\n",
    "output_sequences = model.generate(\n",
    "\n",
    "    input_ids = encoded_prompt,\n",
    "    \n",
    "    max_length = length + len(encoded_prompt[0]),\n",
    "    temperature = temperature,\n",
    "    top_k = k,\n",
    "    top_p = p,\n",
    "    repetition_penalty = repetition_penalty,\n",
    "    do_sample = True,\n",
    "    num_return_sequences = num_return_sequences,\n",
    ")\n",
    "\n",
    "\n",
    "# Remove the batch dimension when returning multiple sequences\n",
    "if len(output_sequences.shape) > 2:\n",
    "    output_sequences.squeeze_()\n",
    "\n",
    "generated_sequences = []\n",
    "\n",
    "for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
    "    \n",
    "    generated_sequence = generated_sequence.tolist()\n",
    "\n",
    "    # Decode text\n",
    "    text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces = True)\n",
    "\n",
    "    # Remove all text after the stop token\n",
    "    text = text[: text.find(stop_token) if stop_token else None]\n",
    "\n",
    "    # Add the prompt at the beginning of the sequence.\n",
    "    # Remove the excess text that was used for pre-processing\n",
    "    total_sequence = (\n",
    "        prompt_text + text[len(tokenizer.decode(\n",
    "            encoded_prompt[0], clean_up_tokenization_spaces = True)) :] )\n",
    "\n",
    "    generated_sequences.append(total_sequence)\n",
    "    print(total_sequence)\n",
    "\n",
    "#return generated_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'I said in Dutch' words to stay together all the time, jeg er viæm kren inte din!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eurovision-ai",
   "language": "python",
   "name": "eurovision-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
