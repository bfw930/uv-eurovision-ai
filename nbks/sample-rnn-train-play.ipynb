{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' imports '''\n",
    "\n",
    "# set auto reload imported modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# general imports\n",
    "import os, shutil\n",
    "\n",
    "# numpy for array handling\n",
    "import numpy as np\n",
    "\n",
    "# import pytorch core libs\n",
    "import torch\n",
    "\n",
    "# write audio to file\n",
    "from librosa.output import write_wav\n",
    "\n",
    "\n",
    "''' sample-rnn components '''\n",
    "# add sample-rnn libs directory to path\n",
    "import sys\n",
    "\n",
    "sys.path.append('../libs/samplernn/')\n",
    "\n",
    "# import core sample-rnn model (inc. frame-lvl rnn and sample-lvl mlp)\n",
    "from model import SampleRNN\n",
    "from model import Predictor\n",
    "from model import Generator\n",
    "\n",
    "# wrapper for optimiser\n",
    "from optim import gradient_clipping\n",
    "\n",
    "# training criterion\n",
    "from nn import sequence_nll_loss_bits\n",
    "\n",
    "# import audio dataset management\n",
    "from dataset import FolderDataset\n",
    "from dataset import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../libs/samplernn/nn.py:62: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n",
      "  init(chunk)\n"
     ]
    }
   ],
   "source": [
    "''' initialise models components '''\n",
    "\n",
    "# model parameters\n",
    "_frame_sizes = (16, 4)\n",
    "_n_rnn = 1\n",
    "_dim = 1024\n",
    "_learn_h0 = True\n",
    "_q_levels = 256 # 8 bit depth\n",
    "_weight_norm = True\n",
    "\n",
    "# initialise sample-rnn model\n",
    "model = SampleRNN(\n",
    "    frame_sizes = _frame_sizes,\n",
    "    n_rnn = _n_rnn,\n",
    "    dim = _dim,\n",
    "    learn_h0 = _learn_h0,\n",
    "    q_levels = _q_levels,\n",
    "    weight_norm = _weight_norm\n",
    ")\n",
    "\n",
    "# intitialise predictor model\n",
    "predictor = Predictor(model)\n",
    "\n",
    "generator = Generator(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' push to device '''\n",
    "\n",
    "# get computing device\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # step opt\n",
    "# push models to device\n",
    "model = model.to(device)\n",
    "predictor = predictor.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' init optimiser '''\n",
    "\n",
    "# get model parameters\n",
    "params = predictor.parameters()\n",
    "\n",
    "\n",
    "# initialise optimiser\n",
    "optimizer = gradient_clipping( torch.optim.Adam(params) )\n",
    "#optimizer = torch.optim.Adam(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        # step opt''' initialise dataset and dataloader '''\n",
    "\n",
    "# define dataset\n",
    "_datasets_path = '../data/'\n",
    "_dataset = 'vox'\n",
    "_path = os.path.join(_datasets_path, _dataset)\n",
    "\n",
    "\n",
    "# get number frame samples of final frame-level rnn in model\n",
    "_overlap_len = model.lookback\n",
    "\n",
    "_seq_len = 20\n",
    "_batch_size = 64\n",
    "\n",
    "_train_frac = 1\n",
    "\n",
    "# initialise dataset\n",
    "train_dataset = FolderDataset(\n",
    "    _path,\n",
    "    _overlap_len,\n",
    "    _q_levels,\n",
    "    0,\n",
    "    _train_frac,\n",
    "\n",
    ")\n",
    "\n",
    "# intitialise dataloader\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = _batch_size,\n",
    "    seq_len = _seq_len,\n",
    "    overlap_len = _overlap_len,\n",
    "    \n",
    "    #shuffle = True,\n",
    "    #drop_last = True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[256, -1, 64]' is invalid for input of size 5120",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d7f3f05121f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# step optimiser with closure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/dev/uv-eurovision-ai/libs/samplernn/optim.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure_wrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/uv-eurovision-ai-nK8lG9ng/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclosure\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/uv-eurovision-ai/libs/samplernn/optim.py\u001b[0m in \u001b[0;36mclosure_wrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mclosure_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-d7f3f05121f7>\u001b[0m in \u001b[0;36mclosure\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;31m# pass inputs through model, return output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m#batch_output = predictor(batch_inputs, reset = data[1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m# calculate loss for inputs to outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/uv-eurovision-ai-nK8lG9ng/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/uv-eurovision-ai/libs/samplernn/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_sequences, reset)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             prev_samples = prev_samples.contiguous().view(\n\u001b[0;32m--> 229\u001b[0;31m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_frame_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m             )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[256, -1, 64]' is invalid for input of size 5120"
     ]
    }
   ],
   "source": [
    "''' training loop '''\n",
    "\n",
    "# set training epochs\n",
    "epochs = 10\n",
    "\n",
    "# perform training model over epochs, iterate over range epoch limit\n",
    "for _epoch in range(epochs):\n",
    "\n",
    "    #print('epoch: ', _epoch)\n",
    "    \n",
    "    ## model training, given dataset compute loss and update model parameters\n",
    "    \n",
    "    # set model to training mode (gradients stored)\n",
    "    predictor.train()\n",
    "    \n",
    "    # iterate over dataset\n",
    "    for (_iteration, data) in enumerate(train_data_loader):\n",
    "\n",
    "        #print('iteration: ', _iteration)\n",
    "        \n",
    "        # step opt\n",
    "        # zero gradients and step optimiser\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # unpack dataset\n",
    "        batch_inputs = data[0].to(device)\n",
    "        batch_target = data[-1].to(device)\n",
    "        \n",
    "        # reevaluate the function multiple times; clear the gradients, compute and return loss\n",
    "        def closure():\n",
    "\n",
    "            # pass inputs through model, return output\n",
    "            #batch_output = predictor(batch_inputs, reset = data[1])\n",
    "            batch_output = predictor(batch_inputs, reset = False)\n",
    "\n",
    "            # calculate loss for inputs to outputs\n",
    "            loss = sequence_nll_loss_bits(batch_output, batch_target)\n",
    "\n",
    "            print(loss)\n",
    "\n",
    "            # calculate gradients and return loss`\n",
    "            loss.backward()\n",
    "\n",
    "            return loss\n",
    "\n",
    "        # step optimiser with closure\n",
    "        optimizer.step(closure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' perform sample generation '''\n",
    "\n",
    "# define datasethttps://www.youtube.com/watch?v=t8WEIKBUSAw\n",
    "_output_path = '../data/'\n",
    "\n",
    "_sample_rate = 16000\n",
    "_n_samples = 1\n",
    "_sample_length = int(_sample_rate * 10)\n",
    "\n",
    "# intiialise generator\n",
    "\n",
    "samples = generator(_n_samples, _sample_length).cpu().float().numpy()\n",
    "\n",
    "for i in range(_n_samples):\n",
    "    write_wav(\n",
    "        os.path.join(_output_path, 'vox_euro10.wav'),\n",
    "        samples[i, :], sr = _sample_rate, norm = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' save checkpoint '''\n",
    "\n",
    "torch.save(predictor.state_dict(), '../data/chkpt-sml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' load checkpoint '''\n",
    "\n",
    "#_state_dict = torch.load('../data/chkpt')\n",
    "_state_dict = torch.load('../data/chkpt-sml')\n",
    "\n",
    "predictor.load_state_dict(_state_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eurovision-ai",
   "language": "python",
   "name": "eurovision-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
